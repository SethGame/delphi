---
description: 
globs: 
alwaysApply: false
---
# Code Patterns and Standards

## Project Structure Patterns

### Chainlit Integration
When working with the main [app.py](mdc:app.py):
- Use `@cl.on_chat_start` for initialization
- Use `@cl.on_message` for handling user messages
- Use `@cl.on_mcp_connect` and `@cl.on_mcp_disconnect` for MCP server management
- Stream responses using `cl.Message` with `stream_token()` and `update()`

### Agent Framework
The project uses OpenAI Agents SDK:
```python
from agents import Agent, Runner
agent = Agent(
    name="apollo",
    instructions="A helpful assistant that can answer questions",
    model=os.getenv("AZURE_OPENAI_DEPLOYMENT"),
    mcp_servers=mcp_servers,
)
```

### LLM Provider Pattern
In [llms/llm_cli.py](mdc:llms/llm_cli.py), the dual provider pattern:
- `generate_azure_completion()` - For Azure OpenAI
- `generate_gemini_completion()` - For Google Gemini  
- `generate_completion()` - Unified interface with provider selection

### Authentication Patterns
- Azure: Uses `EnvironmentCredential` with `get_bearer_token_provider()`
- Gemini: Uses API key from environment variables

## Best Practices
1. Always load environment variables with `load_dotenv()`
2. Use async/await for Chainlit handlers
3. Handle exceptions gracefully in LLM calls
4. Save outputs to the `output/` directory with descriptive filenames
5. Use provider-specific model lists based on selected LLM provider


